{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c81e5174-6fc2-4546-9ad4-24cc56e806f2",
   "metadata": {},
   "source": [
    "# Tutorial 20a: Coherent Spatio-Temporal Scale-separation (CoSTS) with toy data\n",
    "\n",
    "In this tutorial we build a toy data set:\n",
    "- An oscillator with nearly overlapping frequencies between two dynamic systems\n",
    "- An added transient feature.\n",
    "- All influenced by a white noise process.\n",
    "\n",
    "The CoSTS algorithm can be used to separate out each of these individual components, albeit with some hyperparameter tuning. Bad sets of hyperparameters are often indicated by poorly reconstructed windows. However, a sufficiently high level of noise or too large of an initial window can mask the transient feature, which drops out for the next decomposition levels.\n",
    "\n",
    "**Note**: This tutorial walks more explicitly details the mrCOSTS methods by interacting with the lower level functions inside of the COSTS module. This is an approximation of the algorithm inside mrCOSTS. The tutorial is intended to gain intuition of the higher-level mrCOSTS module. The very end of the tutorial shows how to fit using the mrCOSTS module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb58775-955d-49a2-bc01-354d2295f023",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6709fda1-2491-4d9b-b236-c115e7970825",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.conda (Python 3.10.13)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\mmgee\\Documents\\DMV\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# netcdf/numpy/xray/stats\n",
    "import numpy as np\n",
    "import copy\n",
    "import scipy\n",
    "from scipy.integrate import solve_ivp\n",
    "from pydmd.costs import COSTS\n",
    "from pydmd.mrcosts import mrCOSTS\n",
    "\n",
    "# import plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88d429f-7498-4dbb-8a27-4ffcabba5623",
   "metadata": {},
   "source": [
    "## Format plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4f1493-c108-4032-a9de-7d438ef9b2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Higher resolution figures within the notebook\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9edc398-659f-4e6a-9586-f30a7abc9aaf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Set up toy data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17b860b-acbd-4746-b8e5-161e1a9b51a3",
   "metadata": {},
   "source": [
    "## Solve the Overlapping-Scale Oscillators Example\n",
    "\n",
    "- FitzHugh-Nagumo Model\n",
    "- Unforced Duffing Oscillator\n",
    "\n",
    "\n",
    "From Dylewsky et al., (2019):\n",
    "\n",
    "\"The FitzHugh-Nagumo model, used as a simple model for biological neuron dynamics, spikes sharply at intervals determined by its characteristic time scale. The Duffing model, on the other hand, is a simple nonlinear oscillator whose dynamics resemble a distorted sinusoid. Therefore, despite the disparity between τ1 and τ2, the “slow” component periodically acquires a rate of change comparable to that of the “fast” component.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206bbe6b-1050-420a-be4e-57a61994ddef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rhs_FNM(t, x, tau, a, b, Iext):\n",
    "    # FitzHugh-Nagumo Model\n",
    "    v = x[0]\n",
    "    w = x[1]\n",
    "    vdot = v - (v**3) / 3 - w + Iext\n",
    "    wdot = (1 / tau) * (v + a - b * w)\n",
    "    dx = np.array([vdot, wdot])\n",
    "\n",
    "    return dx\n",
    "\n",
    "\n",
    "def rhs_UFD(t, y, eta, epsilon, tau):\n",
    "    # Unforced Duffing Oscillator\n",
    "    p = y[0]\n",
    "    q = y[1]\n",
    "    pdot = q\n",
    "    qdot = (1 / tau) * (-2 * eta * q - p - epsilon * p**3)\n",
    "    dy = np.array([pdot, qdot])\n",
    "\n",
    "    return dy\n",
    "\n",
    "\n",
    "T = 64\n",
    "\n",
    "x0 = np.array([-1.110, -0.125])\n",
    "tau1 = 2\n",
    "a = 0.7\n",
    "b = 0.8\n",
    "Iext = 0.65\n",
    "\n",
    "y0 = np.array([0, 1])\n",
    "eta = 0  # dampling\n",
    "epsilon = 1\n",
    "tau2 = 0.2\n",
    "\n",
    "# RK4 integration of the mixed system\n",
    "dt = 0.0001 * 8\n",
    "# dt = 0.01\n",
    "time = np.arange(0, T, dt)\n",
    "\n",
    "solution_FN = solve_ivp(\n",
    "    rhs_FNM, [0, T], x0, t_eval=time, args=(tau1, a, b, Iext)\n",
    ")\n",
    "\n",
    "solution_UFD = solve_ivp(\n",
    "    rhs_UFD, [0, T], y0, t_eval=time, args=(eta, epsilon, tau2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8137b88-7590-4ab7-bf23-15c92b3a68a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, sharex=True, sharey=True)\n",
    "ax = axes[0]\n",
    "ax.plot(time, solution_FN.y.T)\n",
    "ax.set_title(\"Slow components (FitzHugh-Nagumo)\")\n",
    "ax.set_ylabel(\"Amplitude (-)\")\n",
    "ax.autoscale(enable=True, axis=\"both\", tight=True)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(time, solution_UFD.y.T)\n",
    "ax.set_title(\"Fast components (Unforced Duffing)\")\n",
    "ax.set_ylabel(\"Amplitude (-)\")\n",
    "ax.set_xlabel(\"Time (-)\")\n",
    "ax.autoscale(enable=True, axis=\"both\", tight=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42c14c1-e122-46e0-b0fa-7a21e9c53cdc",
   "metadata": {},
   "source": [
    "## Applying linear mixing of the two non-linear oscillators\n",
    "\n",
    "Here we encounter a difficult problem: the original toy data set has an INCREDIBLY high condition number (~10^15). So much so that perturbations to the problem (e.g., white noise) or changes in hyperparameters (e.g., window length) make the problem very difficult. To mitigate this as much as possible all of the variables output by the solution to the toy data set are used ($p$, $q$, $v$, and $w$) and the toy data are created using an ortho-normal linear mixture. The original example used just two variables and did not use an ortho-normal linear mixture. Thus, the example is slightly different than in the original.\n",
    "\n",
    "However, the toy data system is still not robust to added noise or changes to hyperparameters. Some sets of hyperparameters behave poorly as a result. A good indicator of a poor set of hyperparameters when playing with the hyperparameters is poorly fit windows and bad reconstructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3035e230-15a7-4ec3-b13d-05087d0fd5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified for this toy example to use all components.\n",
    "uv = np.vstack([solution_FN.y, solution_UFD.y]).T\n",
    "\n",
    "# ratio of u and v in linear combination\n",
    "uv_ratio = 1\n",
    "\n",
    "n = np.shape(uv)[1]\n",
    "m = np.shape(uv)[0]\n",
    "\n",
    "# Dimension of space to map into\n",
    "nVars_out = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8646e2-5fe8-413c-a0d4-51e6af9866ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "rng = np.random.default_rng(seed)\n",
    "\n",
    "# Orthonormalized linear mixing matrix\n",
    "Q = scipy.stats.ortho_group.rvs(nVars_out, random_state=seed)\n",
    "Q = Q[0:4, :]\n",
    "\n",
    "A = rng.normal(size=(n, nVars_out))\n",
    "\n",
    "# Here it is important to note the original publication\n",
    "# did not actually perform the orthonormalizing but instead\n",
    "# did the linear mixing with a gaussian-random matrix. We do not\n",
    "# do that here to make the condition number manageable.\n",
    "x = uv @ Q\n",
    "x = x.T\n",
    "\n",
    "print(\n",
    "    \"Conditon number of toy data (smaller is better): {:.2f}\".format(\n",
    "        np.linalg.cond(x)\n",
    "    )\n",
    ")\n",
    "\n",
    "slow_modes = solution_FN.y.T @ Q[0:2, :]\n",
    "fast_modes = solution_UFD.y.T @ Q[2:4, :]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(time, x.T)\n",
    "ax.set_title(\n",
    "    \"Toy data, condition number = {:.0f} (smaller = better)\".format(\n",
    "        np.linalg.cond(x)\n",
    "    )\n",
    ")\n",
    "ax.set_xlabel(\"Time (-)\")\n",
    "ax.set_ylabel(\"Amplitude (-)\")\n",
    "ax.autoscale(enable=True, axis=\"both\", tight=True)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(time, slow_modes, \"k\")\n",
    "ax.plot(time, fast_modes, \"r\")\n",
    "ax.set_title(\"Linear mixtures of fast and slow modes (red=fast, black=slow)\")\n",
    "ax.set_ylabel(\"Amplitude (-)\")\n",
    "ax.autoscale(enable=True, axis=\"both\", tight=True)\n",
    "ax.set_xlabel(\"Time (-)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd60a537-d200-4b09-bf24-91703d24a19a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Add a transient feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff279764-59ad-44c5-ae14-f88905d71a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recon_filter_sd = 15000\n",
    "recon_filter_sd = len(time) * 0.25\n",
    "n_time = x.shape[1]\n",
    "\n",
    "recon_filter = np.exp(\n",
    "    -((np.arange(n_time) - (n_time + 1) / 2) ** 2) / recon_filter_sd**2\n",
    ")\n",
    "recon_filter[recon_filter < 0.0001] = 0\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "f_transient = 10\n",
    "x_transient = (\n",
    "    0.35\n",
    "    * np.sin(f_transient * time.flatten())\n",
    "    * np.sin(0.25 * time.flatten())\n",
    "    * recon_filter\n",
    ")\n",
    "ax.plot(time.flatten(), x_transient.T)\n",
    "ax.set_title(\"Transient feature\")\n",
    "ax.set_ylabel(\"Amplitude (-)\")\n",
    "ax.set_xlabel(\"Time (-)\")\n",
    "ax.autoscale(enable=True, axis=\"both\", tight=True)\n",
    "\n",
    "# Add the transient feature to the data\n",
    "data = x + np.atleast_2d(x_transient)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(time.flatten(), data.T)\n",
    "ax.set_title(\n",
    "    (\n",
    "        \"Toy data + Transient \\n\"\n",
    "        \"condition number = {:.0f} (smaller = better)\".format(\n",
    "            np.linalg.cond(data)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "ax.set_ylabel(\"Amplitude (-)\")\n",
    "ax.set_xlabel(\"Time (-)\")\n",
    "ax.autoscale(enable=True, axis=\"both\", tight=True)\n",
    "\n",
    "print(\n",
    "    \"Conditon number of toy data (smaller is better): {:.2f}\".format(\n",
    "        np.linalg.cond(data)\n",
    "    )\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1787770c-3c8e-4660-8b65-d3b4bc0cd2cf",
   "metadata": {},
   "source": [
    "## Add white noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32aa4d0-b312-4fe2-b77c-2ee30cdfb4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding noise absolutely wrecks the mrd fit. Which is frowing emoji.\n",
    "data = data + rng.normal(0, 0.01, size=(nVars_out, m))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(time.flatten(), data.T)\n",
    "ax.set_title(\n",
    "    (\n",
    "        \"Toy data + Transient + white noise \\n\"\n",
    "        \"condition number = {:.0f} (smaller = better)\".format(\n",
    "            np.linalg.cond(data)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "ax.set_ylabel(\"Amplitude (-)\")\n",
    "ax.set_xlabel(\"Time (-)\")\n",
    "ax.autoscale(enable=True, axis=\"both\", tight=True)\n",
    "\n",
    "\n",
    "print(\n",
    "    \"Conditon number of toy data (smaller is better): {:.2f}\".format(\n",
    "        np.linalg.cond(data)\n",
    "    )\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ef70a0-2328-4ab5-b737-2f79d1a4b38b",
   "metadata": {},
   "source": [
    "# mrCOSTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39c4939-0ebd-45c2-bf58-fa6edd10a04f",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2abd41-a8cd-41ee-a3d1-239529c41d06",
   "metadata": {},
   "source": [
    "### Choosing window sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d35728-7d23-408b-b98e-987fe65b90c5",
   "metadata": {},
   "source": [
    "How big should the window be? This is a non-trivial question. Following the advice in Dylewsky et al., (2019) we should have a window size somewhere around 2 * the period of the fastest feature. \n",
    "\n",
    "When the window is too large and/or the svd rank is too small to fit the data well, these high frequency features are simply dropped. This behavior can be accidentally nice when wanting to ignore high frequency components but frustrating in the cases when these components need to be retained.\n",
    "\n",
    "For more brittle data, i.e. this toy data set, the window size is the most sensitive hyperparameter to tune. Changing the window size to be smaller allows for fitting finer scale features while larger windows tend to focus on larger scale features. \n",
    "\n",
    "Fortunately, for real data with more continuously distributed frequency bands the window size selection is less impactful. Real data with many features present tend to be more forgiving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dd2560-4a9d-40b6-93db-385645d0d361",
   "metadata": {},
   "source": [
    "\n",
    "### Settings\n",
    "- Some of these settings (specifically `global_svd=True`) do not work well on real data but seem to be necessary for this toy data set.\n",
    "- `global_svd=True` is not recommended for real data but is necessary for speeding up the analysis of the toy data in which we have very narrow frequency bands present.\n",
    "- `svd_rank` cannot be larger than the number of spatial dimensions. For 4 spatial dimensions that means we can never resolve more than a low and high frequency scale separation due to the requirement of a conjugate pairs in the eigenvalues.\n",
    "- `transform_method` dictates how the eigenvalue frequencies (the imaginary components) are transformed for the scale-separation step. See the below histograms for an illustration. Generally, for real data we find that `transform_method=\"absolute_value\"` is the more reliable option.\n",
    "- `eig_constraints` can be provided (see `bopdmd` module) using `pydmd_kwargs`. In testing, strict eigenvalue constraints do not perform well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19f132e-f6d5-4228-b104-a5d3a5521fa0",
   "metadata": {},
   "source": [
    "## Drive the recursive decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b60097-852b-4a8e-8ee2-bdab05dfadae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window size (in time steps)\n",
    "window_lengths = np.array([1500, 6000])\n",
    "step_sizes = np.array([100, 400])\n",
    "svd_ranks = [4] * len(window_lengths)\n",
    "num_decompositions = len(window_lengths)\n",
    "mrd_list = []\n",
    "transform_method = \"square_frequencies\"\n",
    "\n",
    "data_iter = np.zeros((num_decompositions, data.shape[0], data.shape[1]))\n",
    "data_iter[0, :, :] = data\n",
    "\n",
    "for n_decomp, (window, step, rank) in enumerate(\n",
    "    zip(window_lengths, step_sizes, svd_ranks)\n",
    "):\n",
    "    print(\"Working on window length={}\".format(window))\n",
    "\n",
    "    x_iter = data_iter[n_decomp, :, :].squeeze()\n",
    "    mrd = COSTS(\n",
    "        svd_rank=rank,\n",
    "        global_svd=True,\n",
    "        pydmd_kwargs={\"eig_constraints\": {\"conjugate_pairs\", \"stable\"}},\n",
    "    )\n",
    "\n",
    "    print(\"Fitting\")\n",
    "    print(\"_________________________________________________\")\n",
    "    mrd.fit(x_iter, np.atleast_2d(time), window, step, verbose=True)\n",
    "    print(\"_________________________________________________\")\n",
    "\n",
    "    # Force the clustering to use two components due to the nature of the toy data.\n",
    "    mrd.cluster_omega(n_components=2, transform_method=transform_method)\n",
    "\n",
    "    # Global reconstruction error indicates if a good set of hyperparameters were chosen.\n",
    "    global_reconstruction = mrd.global_reconstruction()\n",
    "    re = mrd.relative_error(global_reconstruction.real, x_iter)\n",
    "    print(\"Error in Global Reconstruction = {:.2}\".format(re))\n",
    "\n",
    "    # Scale separation\n",
    "    xr_low_frequency, xr_high_frequency = mrd.scale_separation()\n",
    "\n",
    "    # Pass the low frequency component to the next level of decomposition.\n",
    "    if n_decomp < num_decompositions - 1:\n",
    "        data_iter[n_decomp + 1, :, :] = xr_low_frequency\n",
    "\n",
    "    # Save the object for later use.\n",
    "    mrd_list.append(copy.copy(mrd))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b06dc68-a1f7-40a2-a00d-d6e19707ac64",
   "metadata": {},
   "source": [
    "## Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003e018c-7357-412c-93d3-38f59723151c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for nmrd, mrd in enumerate(mrd_list):\n",
    "    # Omega histograms and time series.\n",
    "    window = mrd.window_length\n",
    "    fig1, ax1 = mrd.plot_omega_histogram()\n",
    "    fig1.suptitle(\"Window length={} time steps\".format(window))\n",
    "\n",
    "    fig2, ax2 = mrd.plot_omega_time_series()\n",
    "    fig2.suptitle(\"Window length={} time steps\".format(window))\n",
    "\n",
    "    # Scale separation\n",
    "    xr_sep = mrd.scale_reconstruction()\n",
    "\n",
    "    # Get the input data for this decomposition level.\n",
    "    x_iter = data_iter[nmrd].squeeze()\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        len(np.unique(mrd.omega_classes)) + 1, 1, sharex=True\n",
    "    )\n",
    "    ax = axes[0]\n",
    "    ax.plot(time.squeeze(), x_iter.T, color=\"k\")\n",
    "    ax.set_ylabel(\"Amplitude (-)\")\n",
    "    ax.set_xlabel(\"Time (-)\")\n",
    "    ax.set_title(\"Input Data\")\n",
    "    ax.autoscale(enable=True, axis=\"both\", tight=True)\n",
    "\n",
    "    # Change the underlying data depending on which feature is being separated.\n",
    "    if np.max(mrd.cluster_centroids) > 50:\n",
    "        axes[1].plot(time.squeeze(), x.T, color=\"k\")\n",
    "        axes[1].set_title(\"Reconstruction toy model\")\n",
    "        axes[2].plot(time.squeeze(), x_transient.T, color=\"k\")\n",
    "        axes[2].set_title(\"Reconstruction transient wave packet\")\n",
    "\n",
    "    elif (np.max(mrd.cluster_centroids) > 1) & (\n",
    "        np.max(mrd.cluster_centroids) < 50\n",
    "    ):\n",
    "        axes[1].plot(time.squeeze(), slow_modes, color=\"k\")\n",
    "        axes[1].set_title(\"Reconstruction toy model slow modes\")\n",
    "        axes[2].plot(time.squeeze(), fast_modes, color=\"k\")\n",
    "        axes[2].set_title(\"Reconstruction toy model fast modes\")\n",
    "\n",
    "    for no, o in enumerate(np.unique(mrd.omega_classes)):\n",
    "        ax = axes[no + 1]\n",
    "        ax.plot(time.squeeze(), xr_sep[no, :, :].T, \"r\")\n",
    "        ax.set_ylabel(\"Amplitude (-)\")\n",
    "        ax.set_xlabel(\"Time (-)\")\n",
    "        ax.autoscale(enable=True, axis=\"both\", tight=True)\n",
    "\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206bba9f-fb09-45bf-bd4e-aab2aa184a75",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- The fast and slow modes from the toy data set are even better separated than in the original paper. But, not too surprising given how we engineered this toy data set to be better separable with svd-based methods by reducing the condition number.\n",
    "- Better fast/slow mode separation could probably still be attained by running mrCOSTS at a further decomposition level due to the frequency band leaking (note the wiggles in the slow components of the reconstruction).\n",
    "- The white noise dropped out in the first level, which is a desirable side effect.\n",
    "- The transient feature had to be engineered to be detectable using `global_svd=True`. Otherwise the mrCOSTS fitting is very slow. We achieved this by making it the wave packet very wide. It is possible to isolate more localized transient features with `global_svd=False` but this is prohibitively expenseive for a tutorial.\n",
    "- The enhanced error near the edges of the data can be seen. There is a non-trivial edge effect which grows with increasing `window_length`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57884897-5cad-4121-95ef-4a69246869f1",
   "metadata": {},
   "source": [
    "# How to run the example as mrCOSTS\n",
    "\n",
    "Above we iterated manually through the mrCOSTS algorithm. One can always fit mrCOSTS directly instead, demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65f6c0e-cac1-4717-886d-223bba6b1e9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "window_lengths = np.array([1500, 6000])\n",
    "step_sizes = np.array([100, 400])\n",
    "svd_ranks = [4] * len(window_lengths)\n",
    "transform_method = \"square_frequencies\"\n",
    "n_components_array = [2] * len(window_lengths)\n",
    "global_svd_array = [True] * len(window_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46233634-bbe4-4228-bfc8-6be48b782f2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mrc = mrCOSTS(\n",
    "    svd_rank_array=svd_ranks,\n",
    "    window_length_array=window_lengths,\n",
    "    step_size_array=step_sizes,\n",
    "    global_svd_array=global_svd_array,\n",
    "    n_components_array=n_components_array,\n",
    "    transform_method=transform_method,\n",
    ")\n",
    "\n",
    "mrc.fit(data, np.atleast_2d(time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acfcef2-4e78-4922-921b-17ceae6d0e56",
   "metadata": {},
   "source": [
    "After fitting mrCOSTS you can iterate through the `costs_array` to make the same set of plots. There are also plotting routines available at the mrCOSTS level, detailed in the `costs-tutorial_real-data` notebook. That notebook also details the work flow usually necessary when doing mrCOSTS. These steps were not necessary here for the toy data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
